{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "from skimage import io\n",
    "import time\n",
    "\n",
    "# Data dir\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "DATASET_PATH = os.path.join(ROOT_DIR, \"dataset\")\n",
    "EXPERIMENT_NAME = \"exp-1\"\n",
    "\n",
    "if not os.path.exists(os.path.join(ROOT_DIR, \"logs\")):\n",
    "    os.mkdir(os.path.join(ROOT_DIR, \"logs\"))\n",
    "\n",
    "LOG_PATH = os.path.join(ROOT_DIR, \"logs\", EXPERIMENT_NAME)\n",
    "\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.mkdir(LOG_PATH)\n",
    "    \n",
    "print(os.listdir(DATASET_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_csv(os.path.join(DATASET_PATH, 'metadata.csv'))\n",
    "dt = dt[[\"survival\", \"modality\", \"filename\"]].dropna()\n",
    "dt = dt[dt.modality != \"CT\"]\n",
    "\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "labels = dt[[\"survival\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = dt[\"filename\"].values\n",
    "image_path[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes for data loading and preprocessing\n",
    "class COVIDChestXRayDataset:\n",
    "    def __init__(\n",
    "            self,\n",
    "            datadir,\n",
    "            csv_path,\n",
    "            flag,\n",
    "    ):\n",
    "        \n",
    "        # Patient names in folder\n",
    "        #self.ids = sorted(os.listdir(datadir))\n",
    "        # Sorted patient names in folder\n",
    "        #self.images_fps = [os.path.join(datadir, image_id) for image_id in self.ids]\n",
    "        \n",
    "\n",
    "        # Read csv path\n",
    "        csv = pd.read_csv(os.path.join(csv_path))\n",
    "        csv = csv[[\"survival\", \"modality\", \"filename\"]].dropna()\n",
    "        csv = csv[csv.modality != \"CT\"]\n",
    "        \n",
    "        # Image names\n",
    "        self.image_names = csv[\"filename\"].values\n",
    "        \n",
    "        \n",
    "        # Get labels\n",
    "        self.labels = csv[[\"survival\"]].values\n",
    "        self.image_paths = [os.path.join(datadir, image_id) for image_id in self.image_names]\n",
    "        \n",
    "        # Split\n",
    "        train_vols, test_vols, train_labels, test_labels = train_test_split(self.image_paths, self.labels, test_size=0.20, random_state=42)\n",
    "        print(len(train_vols), len(test_vols))\n",
    "        \n",
    "        self.train_vols = train_vols\n",
    "        self.test_vols = test_vols\n",
    "        self.train_labels = train_labels\n",
    "        self.test_labels = test_labels\n",
    "        \n",
    "        if flag == \"train\":\n",
    "            self.image_paths = self.train_vols\n",
    "            self.labels = self.train_labels\n",
    "            self.ids = self.train_vols\n",
    "            \n",
    "        else:\n",
    "            self.image_paths = self.test_vols\n",
    "            self.labels = self.test_labels\n",
    "            self.ids = self.test_vols\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # Read data\n",
    "        img = io.imread(self.image_paths[i])\n",
    "        img = resize(img, (256, 256))\n",
    "        \n",
    "        img = img.astype(np.float32)\n",
    "        img /= 255\n",
    "        \n",
    "        # Taken from https://github.com/mlmed/torchxrayvision/blob/master/torchxrayvision/datasets.py#L814\n",
    "        # Check that images are 2D arrays\n",
    "        if len(img.shape) > 2:\n",
    "            img = img[:, :, 0]\n",
    "        if len(img.shape) < 2:\n",
    "            print(\"error, dimension lower than 2 for image\")\n",
    "\n",
    "        # Add color channel\n",
    "        img = img[:, :, None]\n",
    "        #img = np.dstack((img, img, img))\n",
    "                   \n",
    "        # Get labels\n",
    "        gt = self.labels[i]\n",
    "        if gt == \"Y\":\n",
    "            gt = 1 # Survival: Yes\n",
    "        else:\n",
    "            gt = 0 # Survial: No\n",
    "            \n",
    "        gt = keras.utils.to_categorical(gt, 2)\n",
    "        \n",
    "        return img, gt\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloder(keras.utils.Sequence):\n",
    "    \"\"\"Load data from dataset and form batches\n",
    "    \n",
    "    Args:\n",
    "        dataset: instance of Dataset class for image loading and preprocessing.\n",
    "        batch_size: Integet number of images in batch.\n",
    "        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(dataset))\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # collect batch data\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "        \n",
    "        # Transpose list of lists\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.indexes) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = os.path.join(DATASET_PATH, \"images/\")\n",
    "CSV_PATH = os.path.join(DATASET_PATH, \"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image path, csv path\n",
    "train_dataset = COVIDChestXRayDataset(IMAGES_PATH, CSV_PATH, flag=\"train\")\n",
    "test_dataset = COVIDChestXRayDataset(IMAGES_PATH, CSV_PATH, flag=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, gt = train_dataset[0] \n",
    "image.shape, gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.squeeze(image)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = Dataloder(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (features, targets) in enumerate(train_dataloader):\n",
    "    print(batch_idx, features.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (features, targets) in enumerate(test_dataloader):\n",
    "    print(batch_idx, features.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Conv2D, Flatten, MaxPool2D, Activation,Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def vgg():\n",
    "    base_model = VGG16(weights=None,include_top=False,pooling='avg',input_shape=(256, 256, 1))\n",
    "    base_model.trainable = False\n",
    "    X = base_model.output\n",
    "    X.trainable = False\n",
    "    predictions = Dense(2, activation='softmax')(X)\n",
    "    \n",
    "    #for layer in base_model.layers:\n",
    "    #    layer.trainable=False\n",
    "        \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        \n",
    "    # Optimzer and loss\n",
    "    optim = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    loss_func = 'binary_crossentropy'\n",
    "    \n",
    "    model.compile(optimizer=optim, loss=loss_func)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "def baseline():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(256, 256, 1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model=None\n",
    "model=vgg()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for learning rate scheduling, logging and best checkpoints saving\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('{}/{}.h5'.format(LOG_PATH, EXPERIMENT_NAME), monitor='val_loss', save_best_only=True, mode='min'),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, verbose=1, patience=5, mode='min'), ## new_lr = lr * factor # 5\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, verbose=1, patience=15, mode='min', restore_best_weights=True), # 8\n",
    "    keras.callbacks.CSVLogger('{}/training.csv'.format(LOG_PATH))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_dataloader, \n",
    "    steps_per_epoch=len(train_dataloader), \n",
    "    epochs=50, \n",
    "    callbacks=callbacks, \n",
    "    validation_data=test_dataloader, \n",
    "    validation_steps=len(test_dataloader),  # val samples = batch size * no of steps\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"--- Time taken to train : %s hours ---\" % ((end_time - start_time)//3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save accuravy loss graphs together\n",
    "def plot_loss(history):\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(loss))\n",
    "    \n",
    "    plt.plot(epochs, loss, 'g')\n",
    "    plt.plot(epochs, val_loss, 'y')\n",
    "    plt.title('Loss')\n",
    "    \n",
    "    plt.ylabel('Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    \n",
    "    plt.legend(['trainloss', 'valloss'], loc='lower right', fontsize=10)\n",
    "    plt.grid(True)\n",
    "    #plt.savefig('{}/{}_acc_loss_graph.jpg'.format(output_path, EXP_NAME), dpi=100)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_test = []\n",
    "\n",
    "for batch_idx, (features, targets) in enumerate(test_dataloader):\n",
    "    \n",
    "    # Make predictions using trained model\n",
    "    y_pred.append(model.predict(features)[0])\n",
    "    y_test.append(targets[0])\n",
    "    \n",
    "y_pred = np.array(y_pred)\n",
    "y_test = np.array(y_test)\n",
    "print(y_pred.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0], y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ground truth to column values\n",
    "y_test_flat = np.argmax(y_test, axis=1)\n",
    "print(\"After flattening ground truth: \", y_test_flat.shape)\n",
    "\n",
    "# Get labels from predictions\n",
    "y_pred_flat = np.array([np.argmax(pred) for pred in y_pred]) # y_pred[1] -> probability for class 1 \n",
    "print(\"Binarize probability values: \", y_pred_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(y_test.shape, y_test_flat.shape, y_pred.shape, y_pred_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "\n",
    "acc = accuracy_score(y_test_flat, y_pred_flat) * 100\n",
    "print(\"Accuracy :\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average precision\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "ap = average_precision_score(y_test, y_pred) * 100\n",
    "print(\"Average precision :\", ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_test_flat, y_pred_flat) \n",
    "print(confusion_mtx)\n",
    "target_names = ['0', '1']\n",
    "print(classification_report(y_test_flat, y_pred_flat, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity and Specificity\n",
    "\n",
    "cm = confusion_matrix(y_pred=y_pred_flat, y_true=y_test_flat)\n",
    "total=sum(sum(cm))\n",
    "\n",
    "sensitivity = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "print('Sensitivity : ', sensitivity*100 )\n",
    "\n",
    "Specificity = cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "print('Specificity : ', Specificity*100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_pred[:10], y_test[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('Area under ROC curve : ', roc_auc_score(y_test, y_pred) *100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    \"\"\"Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    \"\"\"\n",
    "    x = x.copy()\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'th': #keras.backend.image_data_format()\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"Utility function to normalize a tensor by its L2 norm\"\"\"\n",
    "    return (x + 1e-10) / (K.sqrt(K.mean(K.square(x))) + 1e-10)\n",
    "\n",
    "\n",
    "def get_heatmap(gc):\n",
    "  \"\"\"Convert 2D heatmap to 3D for plotting\"\"\"\n",
    "  # Get the color map\n",
    "  cm = plt.get_cmap('jet')\n",
    "  # Apply the colormap like a function to any array:\n",
    "  gc3 = cm(gc)\n",
    "  gc3 = gc3[:, :, :3].astype('float32') \n",
    "  return gc3\n",
    "\n",
    "def grad_cam(input_model, image, cls, layer_name):\n",
    "    \"\"\"GradCAM method for visualizing input saliency.\"\"\"\n",
    "    y_c = input_model.output[0, cls]\n",
    "    conv_output = input_model.get_layer(layer_name).output\n",
    "    grads = K.gradients(y_c, conv_output)[0]\n",
    "    # Normalize if necessary\n",
    "    # grads = normalize(grads)\n",
    "    gradient_function = K.function([input_model.input], [conv_output, grads])\n",
    "\n",
    "    output, grads_val = gradient_function([image])\n",
    "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
    "\n",
    "    weights = np.mean(grads_val, axis=(0, 1))\n",
    "    cam = np.dot(output, weights)\n",
    "\n",
    "    # Process CAM\n",
    "    cam = cv2.resize(cam, (256, 256), cv2.INTER_LINEAR)\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / cam.max()\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "missclass_1 = []\n",
    "\n",
    "for i in tqdm(range(len(y_test_flat))):\n",
    "  # if predicted is 0 and actual is 1\n",
    "  if y_pred_flat[i] == 0 and y_test_flat[i] == 1:\n",
    "    missclass_1.append(x_test[i])\n",
    "\n",
    "# take first 10 missclassified sampels\n",
    "missclass_1 = np.array(missclass_1)\n",
    "\n",
    "print(\"Number of missclassified samples:\", missclass_1.shape)\n",
    "\n",
    "\n",
    "missclass_1_gcam = []\n",
    "\n",
    "#for ms in tqdm(missclass_1):\n",
    "for batch_idx, (features, targets) in enumerate(test_dataloader):\n",
    "    print(features.shape)\n",
    "    features = \n",
    "    ms = np.expand_dims(np.squeeze(features), axis=0)\n",
    "    # grad cam\n",
    "    gc = grad_cam(model, ms, -1, 'block5_conv3')\n",
    "    # convert to 3D\n",
    "    gc = get_heatmap(gc)\n",
    "    missclass_1_gcam.append(gc)\n",
    "\n",
    "missclass_1_gcam = np.array(missclass_1_gcam)\n",
    "\n",
    "print(\"Number of missclassified gradcam samples:\", missclass_1_gcam.shape)\n",
    "\n",
    "\n",
    "from numpy.random import rand\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = np.concatenate((missclass_1, missclass_1_gcam), axis=0)\n",
    "\n",
    "x = results\n",
    "a, b = 1, 10\n",
    "x = np.reshape(x, (a, b, 256, 256, 3))\n",
    "\n",
    "test_data = x\n",
    "r, c = test_data.shape[0], test_data.shape[1]\n",
    "cmaps = [['viridis', 'binary'], ['plasma', 'coolwarm'], ['Greens', 'copper']]\n",
    "\n",
    "heights = [a[0].shape[0] for a in test_data]\n",
    "widths = [a.shape[1] for a in test_data[0]]\n",
    "\n",
    "fig_width = 15.  # inches\n",
    "fig_height = fig_width * sum(heights) / sum(widths)\n",
    "\n",
    "f, axarr = plt.subplots(r,c, figsize=(fig_width, fig_height),\n",
    "        gridspec_kw={'height_ratios':heights})\n",
    "\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        axarr[i, j].imshow(test_data[i][j])\n",
    "        axarr[i, j].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
    "#plt.savefig(\"{}/{}_right_1gradcam.png\".format(base_path, EXP_NAME), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
